Asynchronous Advantage Actor-Critic

# 公式推导

policy gradient中要求的$G_t^n$就是Q-Learning中的Q，即  
$$
\begin{aligned}
E[G_t^n] = Q^{\pi_\theta}(s_t^n, a_t^n)  \\
b = V^{\pi_\theta}(s_t^n)
\end{aligned}
$$

[复习]()中的公式(1)中的“（）”可以写成：   
$$
\begin{aligned}
(\sum_{t'=t}^{Tn}\gamma^{t'-t}r_{t'}^n-b) = Q^{\pi_\theta}(s_t^n, a_t^n) - V^{\pi_\theta}(s_t^n) && (1)
\end{aligned}
$$

结合公式：  
$$
\begin{aligned}
Q^{\pi_\theta}(s_t^n, a_t^n) = r_t^n + V^{\pi_\theta}(s_{t+1}^n)  && (2)
\end{aligned}
$$

公式（2）代入公式（1）得：  
$$
\begin{aligned}
(\sum_{t'=t}^{Tn}\gamma^{t'-t}r_{t'}^n-b) = r_t^n + V^{\pi_\theta}(s_{t+1}^n) - V^{\pi_\theta}(s_t^n) && (3)
\end{aligned}
$$

公式（3）代入$\nabla \bar R_\theta$得：  
$$
\begin{aligned}
\nabla \bar R_\theta \approx \frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{Tn}\left(r_t^n + V^{\pi_\theta}(s_{t+1}^n) - V^{\pi_\theta}(s_t^n)\right)\nabla\log p_\theta(a_t^n|s_t^n)   && (4)
\end{aligned}
$$

# 训练过程

1. 用$\pi$与环境互动，sample出labelled data  
2. 用TD或MC，基于labelled data学习$V^{\pi_\theta}(s)$  
3. 根据公式$V^{\pi_\theta}(s)$更新$\pi$  
![](/assets/images/RL/1.png)    

注意：  
对$V^{\pi}(s)$增加一个正则化，使$V^{\pi}(s)$的entropy倾向于larger  

# Asynchronous  
NN开多个影分身同时修行加快训练速度  
![](/assets/images/RL/2.png)    
for every worker:  
1. copy global 参数  
2. 独立地sample data并计算$\nabla \theta$  
3. 把$\nabla \theta$传到global  
4. global更新参数