# 复习policy Gradient：  

$$
\begin{aligned}
\nabla \bar R_\theta \approx \frac{1}{N}\sum_{n=1}^N\sum_{t=1}^{Tn}\left(\sum_{t'=t}^{Tn}\gamma^{t'-t}r_{t'}^n-b\right)\nabla\log p_\theta(a_t^n|s_t^n)   && (1)
\end{aligned}
$$

公式解释：  
$p_\theta(a_t^n|s_t^n)$：在某个state采取某个Action的几率  
$\nabla \bar R_\theta$：在state上采取action后，到游戏结束，得到的reward的期望  
t：从现在开始的时间点   
$\gamma^{t'-t}$：t时刻的action对t'时刻的影响力  
$r_{t'}^n$：t'时刻得到的reward  
b：baseline，用于保证（）有正有负  
如果()为正，就要$p_\theta(a_t^n|s_t^n)\uparrow$，否则$p_\theta(a_t^n|s_t^n)\downarrow$  

定义：  
$$
G_t^n = \sum_{t'=t}^{T_n}\gamma^{t'-t}r_{t'}^n
$$

其中$r_{t'}^n$通过与环境互动得到，由于游戏的随机性，这使得$G_t^n$非常不稳定。 
怎样让$G_t^n$变得稳定？  
答：用NN估计$G_t^n$而不是与环境真实互动。  

# 复习Q-Learning

定义 state value function $V^\pi(s)$：对于一个特定的$\pi$，输入当前state $s$，输出期望的cumulated reward。   
定义 state-action value function $Q^\pi(s, a)$：对于一个特定的$\pi$，输入当前state $s$，**并强制采取动作a**，输出期望的cumulated reward。    

